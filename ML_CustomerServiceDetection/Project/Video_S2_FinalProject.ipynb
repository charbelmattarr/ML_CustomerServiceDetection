{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPoNz1RDkToS9dYNzyMA898"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VM32VRTlK0Ty","executionInfo":{"status":"ok","timestamp":1710643121416,"user_tz":-120,"elapsed":22033,"user":{"displayName":"charbel mattar","userId":"12792791886941109691"}},"outputId":"6a03ba2d-9d4b-41b5-b443-ad6d486fec8d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import cv2\n","from PIL import Image\n","import torch\n","from transformers import DetrForObjectDetection, DetrImageProcessor, AutoImageProcessor, AutoModelForImageClassification\n","from google.colab.patches import cv2_imshow\n","\n","# Initialize the DETR processor and model for object detection\n","object_detection_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n","object_detection_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n","\n","# Initialize the emotion detection model\n","model_path = \"/content/drive/MyDrive/CharbelMATTAR_LU_WebDev/Project/Model\"\n","emotion_processor = AutoImageProcessor.from_pretrained(\"model_path\")\n","model_path = \"/content/drive/MyDrive/CharbelMATTAR_LU_WebDev/Project/Model\"\n","emotion_model = AutoModelForImageClassification.from_pretrained(\"model_path\")\n","\n","# Open the video file\n","video_path = \"/content/drive/MyDrive/Videos/CustomerService6.mp4\"\n","video_capture = cv2.VideoCapture(video_path)\n","\n","# Define the codec and create VideoWriter object\n","fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n","frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n","frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","out = cv2.VideoWriter('/content/drive/MyDrive/Videos/output_video5.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width,frame_height))\n","\n","\n","while video_capture.isOpened():\n","\n","    person_counter = 0\n","    customer_counter = 0\n","    sales_counter = 0\n","    ret, frame = video_capture.read()\n","    if not ret:\n","        break\n","\n","\n","    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","\n","\n","    inputs = object_detection_processor(images=frame_pil, return_tensors=\"pt\")\n","    object_outputs = object_detection_model(**inputs)\n","\n","\n","    target_sizes = torch.tensor([frame_pil.size[::-1]])\n","    object_results = object_detection_processor.post_process_object_detection(object_outputs, target_sizes=target_sizes, threshold=0.9)[0]\n","\n","\n","    midpoint = frame_width // 2\n","\n","    # Visualize the frame with bounding boxes\n","    for score, label, box in zip(object_results[\"scores\"], object_results[\"labels\"], object_results[\"boxes\"]):\n","        box = [round(i, 2) for i in box.tolist()]\n","        class_name = object_detection_model.config.id2label[label.item()]\n","        if class_name == \"person\":\n","            person_counter += 1\n","\n","            person_midpoint = (box[0] + box[2]) // 2\n","            if person_midpoint < midpoint:\n","                sales_counter += 1\n","                person_label = \"Sales Rep\"\n","\n","            else:\n","                customer_counter += 1\n","                person_label = \"Customer\"\n","\n","\n","            roi = frame[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n","\n","\n","            emotion_inputs = emotion_processor(roi, return_tensors=\"pt\")\n","\n","            emotion_outputs = emotion_model(**emotion_inputs)\n","\n","            predicted_probabilities = torch.softmax(emotion_outputs.logits, dim=1)\n","\n","            predicted_emotion_index = torch.argmax(predicted_probabilities, dim=1)\n","            predicted_emotion_class = predicted_emotion_index.item()\n","\n","            # Map the predicted index to the corresponding emotion label\n","            emotion_labels = ['Ahegao', 'Angry', 'Happy', 'Neutral', 'Sad', 'Surprise']\n","            predicted_emotion = emotion_labels[predicted_emotion_class]\n","            if person_midpoint < midpoint:\n","                label_string = f\"{person_label} {sales_counter}: {round(score.item(), 3)}, {predicted_emotion}\"\n","            else:\n","                label_string = f\"{person_label} {customer_counter}: {round(score.item(), 3)}, {predicted_emotion}\"\n","\n","\n","            # Add bounding box and label\n","            cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255, 0, 0), 2)\n","            cv2.putText(frame, label_string, (int(box[0]), int(box[1]) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n","        elif class_name == \"cell phone\":\n","            label_string = f\"{class_name}: {round(score.item(), 3)}\"\n","            cv2.putText(frame, label_string, (int(box[0]), int(box[1]) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","\n","    # Display the resulting frame\n","    cv2_imshow(frame)\n","    cv2.waitKey(1)\n","    # Write the frame into the output video\n","    out.write(frame)\n","\n","# Release the video capture object and the output video writer\n","video_capture.release()\n","out.release()\n","\n","# Close all the frames\n","cv2.destroyAllWindows()\n"],"metadata":{"id":"gi9iBePfUTk1","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ZFzMG7qwTCityJkadzmzK44Mj3qoB96t"},"executionInfo":{"status":"ok","timestamp":1710643960468,"user_tz":-120,"elapsed":784844,"user":{"displayName":"charbel mattar","userId":"12792791886941109691"}},"outputId":"fef3b968-7560-41dc-e6e3-242ac04b0c58"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}